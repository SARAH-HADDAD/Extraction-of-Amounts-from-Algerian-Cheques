# -*- coding: utf-8 -*-
"""script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jQIGgw-3h77HADI00cRH0CdvA5SK9X3Q

Entraîner TrOCR sur un ensemble de données personnalisé de chèques pourrait pour réduire les erreurs.
"""

!pip install -q transformers
!pip install -q sentencepiece
!pip install -q jiwer
!pip install -q datasets
!pip install -q evaluate
!pip install -q -U accelerate
!pip install -q matplotlib
!pip install -q protobuf==3.20.1
!pip install -q tensorboard

"""transformers : Il s'agit de la bibliothèque Hugging Face transformers qui nous donne accès à des centaines de modèles basés sur des transformateurs, y compris le modèle TrOCR.

sentencepiece : Il s'agit de la bibliothèque de tokenisation sentencepiece nécessaire pour convertir les mots en tokens et nombres. C'est également une partie de la famille Hugging Face.

jiwer : La bibliothèque jiwer nous donne accès à plusieurs métriques de reconnaissance vocale et de langage. Celles-ci incluent le WER (Word Error Rate) et le CER (Character Error Rate). Nous utiliserons la métrique CER pour évaluer le modèle pendant l'entraînement.
"""

import os
import os
import torch
import evaluate
import numpy as np
import pandas as pd
import glob as glob
import torch.optim as optim
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import torchvision.transforms.functional as F
import random
from PIL import Image
from zipfile import ZipFile
from tqdm.notebook import tqdm
from dataclasses import dataclass
from torch.utils.data import Dataset
from urllib.request import urlretrieve
from transformers import (
    VisionEncoderDecoderModel,
    TrOCRProcessor,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    GenerationConfig
)

"""VisionEncoderDecoderModel : Nous avons besoin de cette classe pour définir différents modèles TrOCR.

TrOCRProcessor : TrOCR attend que le jeu de données suive un processus de normalisation particulier. Cette classe veillera à ce que les images soient correctement normalisées et traitées.

Seq2SeqTrainer : Ceci est nécessaire pour initialiser l'API Trainer.

Seq2SeqTrainingArguments : Pendant l'entraînement, l'API Trainer attend plusieurs arguments.

La classe Seq2SeqTrainingArguments initialise tous les arguments requis avant de les passer à l'API.

transforms : Le module de transformations Torchvision est nécessaire pour appliquer des augmentations de données aux images.

"""

# the seed for reproducibility across different runs and define the computation device.

def seed_everything(seed_value):
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed_everything(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/TrOCR

!ls fr

@dataclass(frozen=True)
class TrainingConfig:
    BATCH_SIZE:    int = 8
    EPOCHS:        int = 50
    LEARNING_RATE: float = 0.0005

@dataclass(frozen=True)
class DatasetConfig:
    DATA_ROOT:     str = 'fr'

@dataclass(frozen=True)
class ModelConfig:
    MODEL_NAME: str = 'microsoft/trocr-base-handwritten'

def is_image_file(filename):
    # Check if the file extension is a common image format
    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))

# Load the CSV into a DataFrame
dataset_path=DatasetConfig.DATA_ROOT

train_df = pd.read_csv(f"{dataset_path}/train/train.csv")

# Rename columns
train_df.rename(columns={'file-name': 'file_name'}, inplace=True)

valid_df = pd.read_csv(f"{dataset_path}/valid/valid.csv")

# Rename columns
valid_df.rename(columns={'file-name': 'file_name'}, inplace=True)

print("Valid DataFrame:")
print(valid_df.head())

print("Train DataFrame:")
print(train_df.head())

# defining the augmentations

class VerySmallRotation(object):
    def __init__(self, max_degrees=1):
        self.max_degrees = max_degrees

    def __call__(self, img):
        angle = random.uniform(-self.max_degrees, self.max_degrees)
        return F.rotate(img, angle, fill=(255, 255, 255))

class MinimalShear(object):
    def __init__(self, max_shear=0.02):
        self.max_shear = max_shear

    def __call__(self, img):
        shear = random.uniform(-self.max_shear, self.max_shear)
        return F.affine(img, angle=0, translate=(0, 0), scale=1, shear=[shear, 0], fill=(255, 255, 255))

train_transforms = transforms.Compose([
    transforms.RandomApply([
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    ], p=0.3),
    transforms.RandomApply([
        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),
    ], p=0.1),
    VerySmallRotation(max_degrees=1),
    MinimalShear(max_shear=0.02),
    transforms.RandomApply([
        transforms.Lambda(lambda x: F.adjust_gamma(x, gamma=random.uniform(0.9, 1.1))),
    ], p=0.2),
])

class CustomOCRDataset(Dataset):
    def __init__(self, root_dir, df, processor, max_target_length=128):
        self.root_dir = root_dir
        self.df = df
        self.processor = processor
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # The image file name.
        file_name = self.df['file_name'][idx]
        # The text (label).
        text = self.df['text'][idx]
        # Read the image, apply augmentations, and get the transformed pixels.
        image = Image.open(os.path.join(self.root_dir, file_name)).convert('RGB')
        image = train_transforms(image)
        pixel_values = self.processor(image, return_tensors='pt').pixel_values
        # Pass the text through the tokenizer and get the labels,
        # i.e. tokenized labels.
        labels = self.processor.tokenizer(
            text,
            padding='max_length',
            max_length=self.max_target_length
        ).input_ids
        # We are using -100 as the padding token.
        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]
        encoding = {"pixel_values": pixel_values.squeeze(), "labels": torch.tensor(labels)}
        return encoding

"""The __init__() method accepts the root directory path, the DataFrame, TrOCR processor, and the maximum label length as parameters.

The __getitem__() method first reads the label and image from the disk. It then passes the image through the transforms to apply the augmentations. The TrOCRProcessor returns the normalized pixel values in PyTorch tensor format. Next, the text labels are passed through the tokenizer. If a label is shorter than 128 characters, it is padded with -100 to a length of 128. If it is longer than 128 characters, then the characters are truncated.  Finally, it returns the pixel values and the labels as a dictionary.
"""

processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)
train_dataset = CustomOCRDataset(
    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'train/'),
    df=train_df,
    processor=processor
)
valid_dataset = CustomOCRDataset(
    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'valid/'),
    df=valid_df,
    processor=processor
)

print("Number of training examples:", len(train_dataset))
print("Number of validation examples:", len(valid_dataset))

# Charger le modèle pré-entraîné
model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)
model.to(device)
print(model)
# Total parameters and trainable parameters.
total_params = sum(p.numel() for p in model.parameters())
print(f"{total_params:,} total parameters.")
total_trainable_params = sum(
    p.numel() for p in model.parameters() if p.requires_grad)
print(f"{total_trainable_params:,} training parameters.")

# Set special tokens used for creating the decoder_input_ids from the labels.
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
# make sure vocab size is set correctly
model.config.vocab_size = model.config.decoder.vocab_size
# set beam search parameters
model.config.eos_token_id = processor.tokenizer.sep_token_id
model.config.max_length = 64
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4

optimizer = optim.AdamW(
    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.005
)

cer_metric = evaluate.load('cer')

def compute_cer(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    return {"cer": cer}

# Définir les arguments d'entraînement
training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy='epoch',
    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,
    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,
    fp16=True,
    output_dir='./',
    logging_steps=2,
    save_steps=100,
    eval_steps=100,
)

# Initialize trainer.
trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=processor.feature_extractor,
    args=training_args,
    compute_metrics=compute_cer,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    data_collator=default_data_collator
)

res = trainer.train()

processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)
trained_model = VisionEncoderDecoderModel.from_pretrained('seq2seq_model_printed/checkpoint-'+str(res.global_step)).to(device)

def read_and_show(image_path):
    """
    :param image_path: String, path to the input image.


    Returns:
        image: PIL Image.
    """
    image = Image.open(image_path).convert('RGB')
    return image

def ocr(image, processor, model):
    """
    :param image: PIL Image.
    :param processor: Huggingface OCR processor.
    :param model: Huggingface OCR model.


    Returns:
        generated_text: the OCR'd text string.
    """
    # We can directly perform OCR on cropped images.
    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device)
    generated_ids = model.generate(pixel_values)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return generated_text

def eval_new_data(
    data_path=os.path.join(DatasetConfig.DATA_ROOT, 'valid', '*'),
    num_samples=60
):
    image_paths = glob.glob(data_path)
    image_paths = [path for path in image_paths if is_image_file(path)]  # Filter out non-image files
    for i, image_path in tqdm(enumerate(image_paths), total=len(image_paths)):
        if i == num_samples:
            break
        image = read_and_show(image_path)
        text = ocr(image, processor, trained_model)
        plt.figure(figsize=(7, 4))
        plt.imshow(image)
        plt.title(text)
        plt.axis('off')
        plt.show()

eval_new_data(
    data_path=os.path.join(DatasetConfig.DATA_ROOT, 'valid', '*'),
    num_samples=60
)