{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9429405,"sourceType":"datasetVersion","datasetId":5728480}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers\n!pip install -q sentencepiece\n!pip install -q jiwer\n!pip install -q datasets\n!pip install -q evaluate\n!pip install -q -U accelerate\n!pip install -q matplotlib\n!pip install -q protobuf==3.20.1\n!pip install -q tensorboard","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T18:41:47.676040Z","iopub.execute_input":"2024-09-18T18:41:47.676934Z","iopub.status.idle":"2024-09-18T18:43:52.474407Z","shell.execute_reply.started":"2024-09-18T18:41:47.676891Z","shell.execute_reply":"2024-09-18T18:43:52.473256Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport os\nimport torch\nimport evaluate\nimport numpy as np\nimport pandas as pd\nimport glob as glob\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as F\nimport random\nfrom PIL import Image\nfrom zipfile import ZipFile\nfrom tqdm.notebook import tqdm\nfrom dataclasses import dataclass\nfrom torch.utils.data import Dataset\nfrom urllib.request import urlretrieve\nfrom transformers import (\n    VisionEncoderDecoderModel,\n    TrOCRProcessor,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    default_data_collator,\n    GenerationConfig\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.477128Z","iopub.execute_input":"2024-09-18T18:43:52.477596Z","iopub.status.idle":"2024-09-18T18:43:52.486376Z","shell.execute_reply.started":"2024-09-18T18:43:52.477546Z","shell.execute_reply":"2024-09-18T18:43:52.485401Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed_value):\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.487626Z","iopub.execute_input":"2024-09-18T18:43:52.487989Z","iopub.status.idle":"2024-09-18T18:43:52.503940Z","shell.execute_reply.started":"2024-09-18T18:43:52.487956Z","shell.execute_reply":"2024-09-18T18:43:52.503118Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass TrainingConfig:\n    BATCH_SIZE:    int = 16\n    EPOCHS:        int = 100\n    LEARNING_RATE: float = 0.0005\n\n@dataclass(frozen=True)\nclass DatasetConfig:\n    DATA_ROOT:     str = '/kaggle/input/amount-in-french/fr'\n\n@dataclass(frozen=True)\nclass ModelConfig:\n    MODEL_NAME: str = 'microsoft/trocr-base-handwritten'","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.505981Z","iopub.execute_input":"2024-09-18T18:43:52.506330Z","iopub.status.idle":"2024-09-18T18:43:52.517721Z","shell.execute_reply.started":"2024-09-18T18:43:52.506279Z","shell.execute_reply":"2024-09-18T18:43:52.516703Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"dataset_path=\"/kaggle/input/amount-in-french/fr\"\n\ntrain_df = pd.read_csv(f\"{dataset_path}/train/train.csv\")\n\ntrain_df.rename(columns={'file-name': 'file_name'}, inplace=True)\n\nvalid_df = pd.read_csv(f\"{dataset_path}/valid/valid.csv\")\n\nvalid_df.rename(columns={'file-name': 'file_name'}, inplace=True)\n\nprint(\"Valid DataFrame:\")\nprint(valid_df.head())\n\nprint(\"Train DataFrame:\")\nprint(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.518957Z","iopub.execute_input":"2024-09-18T18:43:52.519288Z","iopub.status.idle":"2024-09-18T18:43:52.549903Z","shell.execute_reply.started":"2024-09-18T18:43:52.519247Z","shell.execute_reply":"2024-09-18T18:43:52.548950Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Valid DataFrame:\n    file_name                                              text\n0  000000.png                                millions de dinars\n1  000040.png                                       Vingt mille\n2  000077.png                                   cent dix dinars\n3  000137.png                                   SIX CENT TRENTE\n4  000391.png  cent vingt-trois mille quatre cent cinquante-six\nTrain DataFrame:\n    file_name                     text\n0  000225.png                cinquante\n1  000258.png                    douze\n2  000158.png                       un\n3  000109.png  Vingt six milles dinars\n4  000023.png       cent Vingts Dinars\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining the augmentations\n\nclass VerySmallRotation(object):\n    def __init__(self, max_degrees=1):\n        self.max_degrees = max_degrees\n\n    def __call__(self, img):\n        angle = random.uniform(-self.max_degrees, self.max_degrees)\n        return F.rotate(img, angle, fill=(255, 255, 255))\n\nclass MinimalShear(object):\n    def __init__(self, max_shear=0.02):\n        self.max_shear = max_shear\n\n    def __call__(self, img):\n        shear = random.uniform(-self.max_shear, self.max_shear)\n        return F.affine(img, angle=0, translate=(0, 0), scale=1, shear=[shear, 0], fill=(255, 255, 255))\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomApply([\n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n    ], p=0.3),\n    transforms.RandomApply([\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n    ], p=0.1),\n    VerySmallRotation(max_degrees=1),\n    MinimalShear(max_shear=0.02),\n    transforms.RandomApply([\n        transforms.Lambda(lambda x: F.adjust_gamma(x, gamma=random.uniform(0.9, 1.1))),\n    ], p=0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.551247Z","iopub.execute_input":"2024-09-18T18:43:52.551764Z","iopub.status.idle":"2024-09-18T18:43:52.564773Z","shell.execute_reply.started":"2024-09-18T18:43:52.551694Z","shell.execute_reply":"2024-09-18T18:43:52.563825Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class CustomOCRDataset(Dataset):\n    def __init__(self, root_dir, df, processor, max_target_length=128):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # The image file name.\n        file_name = self.df['file_name'][idx]\n        # The text (label).\n        text = self.df['text'][idx]\n        # Read the image, apply augmentations, and get the transformed pixels.\n        image = Image.open(os.path.join(self.root_dir, file_name)).convert('RGB')\n        image = train_transforms(image)\n        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n        # Pass the text through the tokenizer and get the labels,\n        # i.e. tokenized labels.\n        labels = self.processor.tokenizer(\n            text,\n            padding='max_length',\n            max_length=self.max_target_length\n        ).input_ids\n        # We are using -100 as the padding token.\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.565979Z","iopub.execute_input":"2024-09-18T18:43:52.566322Z","iopub.status.idle":"2024-09-18T18:43:52.577428Z","shell.execute_reply.started":"2024-09-18T18:43:52.566285Z","shell.execute_reply":"2024-09-18T18:43:52.576516Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\ntrain_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'train/'),\n    df=train_df,\n    processor=processor\n)\nvalid_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'valid/'),\n    df=valid_df,\n    processor=processor\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:52.578647Z","iopub.execute_input":"2024-09-18T18:43:52.578968Z","iopub.status.idle":"2024-09-18T18:43:55.186657Z","shell.execute_reply.started":"2024-09-18T18:43:52.578932Z","shell.execute_reply":"2024-09-18T18:43:55.185388Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7cee52e3a94ec7892b77ddf3ebee63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76246661bec44e48b87e0efb1f1f506f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046d29ae38f445d7868d9c169df40163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34628c813b324abb94669323e7cdbb19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea65604db48343ecacb1d2ec756cb9a3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(valid_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:49:58.094171Z","iopub.execute_input":"2024-09-18T18:49:58.095120Z","iopub.status.idle":"2024-09-18T18:49:58.099992Z","shell.execute_reply.started":"2024-09-18T18:49:58.095061Z","shell.execute_reply":"2024-09-18T18:49:58.099042Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Number of training examples: 340\nNumber of validation examples: 61\n","output_type":"stream"}]},{"cell_type":"code","source":"# Charger le modèle pré-entraîné\nmodel = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\nmodel.to(device)\nprint(model)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:43:55.188215Z","iopub.execute_input":"2024-09-18T18:43:55.188712Z","iopub.status.idle":"2024-09-18T18:44:07.283565Z","shell.execute_reply.started":"2024-09-18T18:43:55.188657Z","shell.execute_reply":"2024-09-18T18:44:07.282503Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb6531ebadd644889b44beaccc976bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91db38f450c440f2b812caa6947b7396"}},"metadata":{}},{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb54c60b9df4db488dd46383eca2064"}},"metadata":{}},{"name":"stdout","text":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)\n333,921,792 total parameters.\n333,921,792 training parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set special tokens used for creating the decoder_input_ids from the labels.\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n# set beam search parameters\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:07.287765Z","iopub.execute_input":"2024-09-18T18:44:07.288053Z","iopub.status.idle":"2024-09-18T18:44:07.293904Z","shell.execute_reply.started":"2024-09-18T18:44:07.288021Z","shell.execute_reply":"2024-09-18T18:44:07.292944Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.AdamW(\n    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.005\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:07.295394Z","iopub.execute_input":"2024-09-18T18:44:07.295834Z","iopub.status.idle":"2024-09-18T18:44:12.516412Z","shell.execute_reply.started":"2024-09-18T18:44:07.295783Z","shell.execute_reply":"2024-09-18T18:44:12.515124Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"cer_metric = evaluate.load('cer')\n\ndef compute_cer(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n    return {\"cer\": cer}","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:12.517741Z","iopub.execute_input":"2024-09-18T18:44:12.518099Z","iopub.status.idle":"2024-09-18T18:44:13.076955Z","shell.execute_reply.started":"2024-09-18T18:44:12.518056Z","shell.execute_reply":"2024-09-18T18:44:13.076145Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Définir les arguments d'entraînement\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    eval_strategy='epoch',\n    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n    fp16=True,\n    output_dir='seq2seq_model_printed/',\n    logging_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=5,\n    report_to='tensorboard',\n    num_train_epochs=TrainingConfig.EPOCHS\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:13.078126Z","iopub.execute_input":"2024-09-18T18:44:13.078467Z","iopub.status.idle":"2024-09-18T18:44:13.114369Z","shell.execute_reply.started":"2024-09-18T18:44:13.078433Z","shell.execute_reply":"2024-09-18T18:44:13.113462Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Initialize trainer.\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=default_data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:53:27.666473Z","iopub.execute_input":"2024-09-18T18:53:27.666916Z","iopub.status.idle":"2024-09-18T18:53:27.685961Z","shell.execute_reply.started":"2024-09-18T18:53:27.666877Z","shell.execute_reply":"2024-09-18T18:53:27.685058Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:137: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"res = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:53:36.817319Z","iopub.execute_input":"2024-09-18T18:53:36.817740Z","iopub.status.idle":"2024-09-18T18:53:39.489987Z","shell.execute_reply.started":"2024-09-18T18:53:36.817699Z","shell.execute_reply":"2024-09-18T18:53:39.488601Z"},"trusted":true},"execution_count":44,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2289\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2289\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2292\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2295\u001b[0m ):\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3328\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3328\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3333\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3334\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3373\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3372\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3373\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3375\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py\", line 634, in forward\n    loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.reshape(-1))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1188, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3104, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 328.12 MiB is free. Process 5018 has 14.42 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 367.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py\", line 634, in forward\n    loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.reshape(-1))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1188, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3104, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 328.12 MiB is free. Process 5018 has 14.42 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 367.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"error"}]},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\ntrained_model = VisionEncoderDecoderModel.from_pretrained('seq2seq_model_printed/checkpoint-'+str(res.global_step)).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:17.267736Z","iopub.status.idle":"2024-09-18T18:44:17.268118Z","shell.execute_reply.started":"2024-09-18T18:44:17.267929Z","shell.execute_reply":"2024-09-18T18:44:17.267947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_and_show(image_path):\n    \"\"\"\n    :param image_path: String, path to the input image.\n\n\n    Returns:\n        image: PIL Image.\n    \"\"\"\n    image = Image.open(image_path).convert('RGB')\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:17.269879Z","iopub.status.idle":"2024-09-18T18:44:17.270259Z","shell.execute_reply.started":"2024-09-18T18:44:17.270070Z","shell.execute_reply":"2024-09-18T18:44:17.270089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ocr(image, processor, model):\n    \"\"\"\n    :param image: PIL Image.\n    :param processor: Huggingface OCR processor.\n    :param model: Huggingface OCR model.\n\n\n    Returns:\n        generated_text: the OCR'd text string.\n    \"\"\"\n    # We can directly perform OCR on cropped images.\n    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device)\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:17.271878Z","iopub.status.idle":"2024-09-18T18:44:17.272253Z","shell.execute_reply.started":"2024-09-18T18:44:17.272072Z","shell.execute_reply":"2024-09-18T18:44:17.272092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_image_file(filename):\n    # Check if the file extension is a common image format\n    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n\ndef eval_new_data(\n    data_path=os.path.join(DatasetConfig.DATA_ROOT, 'valid', '*'),\n    num_samples=90\n):\n    image_paths = glob.glob(data_path)\n    image_paths = [path for path in image_paths if is_image_file(path)]  # Filter out non-image files\n    for i, image_path in tqdm(enumerate(image_paths), total=len(image_paths)):\n        if i == num_samples:\n            break\n        image = read_and_show(image_path)\n        text = ocr(image, processor, trained_model)\n        plt.figure(figsize=(7, 4))\n        plt.imshow(image)\n        plt.title(text)\n        plt.axis('off')\n        plt.show()\n\neval_new_data(\n    data_path=os.path.join(DatasetConfig.DATA_ROOT, 'valid', '*'),\n    num_samples=90\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T18:44:17.274482Z","iopub.status.idle":"2024-09-18T18:44:17.274917Z","shell.execute_reply.started":"2024-09-18T18:44:17.274727Z","shell.execute_reply":"2024-09-18T18:44:17.274747Z"},"trusted":true},"execution_count":null,"outputs":[]}]}